{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DasBytes/three-stage-banglish-depression-classifier/blob/main/ANN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UugM4dlKm7mp",
        "outputId": "8a432962-5fb6-4d0e-e403-235c2d0f795a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/608.4 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import classification_report, accuracy_score, f1_score, precision_score, recall_score\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "\n",
        "try:\n",
        "    nltk.download('stopwords')\n",
        "except:\n",
        "    print(\"NLTK stopwords already downloaded.\")\n",
        "\n",
        "try:\n",
        "    df = pd.read_csv(\"Banglish depression dataset.csv\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: 'Banglish depression dataset.csv' not found. Please ensure the file is in the correct directory.\")\n",
        "    exit()\n",
        "\n",
        "df.dropna(subset=['Sentence', 'Category'], inplace=True)\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english'))\n",
        "stopwords_bangla = {'ami','tumi','shei','amra','eto','kemon','achho','aschi','na'}\n",
        "all_stopwords = stopwords_eng.union(stopwords_bangla)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in all_stopwords]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['Cleaned_Sentence'] = df['Sentence'].apply(clean_text)\n",
        "\n",
        "label_map = {\"No Depression\": 0, \"Mild\": 1, \"Severe\": 2}\n",
        "df['Label'] = df['Category'].map(label_map)\n",
        "\n",
        "if df['Label'].isnull().any():\n",
        "    df.dropna(subset=['Label'], inplace=True)\n",
        "    df['Label'] = df['Label'].astype(int)\n",
        "else:\n",
        "    df['Label'] = df['Label'].astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    df['Cleaned_Sentence'], df['Label'], test_size=0.2, random_state=42, stratify=df['Label']\n",
        ")\n",
        "\n",
        "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,3))\n",
        "X_train_tfidf = tfidf.fit_transform(X_train)\n",
        "X_test_tfidf = tfidf.transform(X_test)\n",
        "\n",
        "mlp = MLPClassifier(hidden_layer_sizes=(128,64), activation='relu', solver='adam', max_iter=300,\n",
        "                    early_stopping=True, random_state=42, verbose=True)\n",
        "\n",
        "print(\"\\n\" + \"=\"*30)\n",
        "print(\"STARTING MLP TRAINING\")\n",
        "print(\"=\"*30)\n",
        "\n",
        "mlp.fit(X_train_tfidf, y_train)\n",
        "\n",
        "print(\"=\"*30)\n",
        "print(\"MLP TRAINING COMPLETE\")\n",
        "print(\"=\"*30 + \"\\n\")\n",
        "\n",
        "y_pred = mlp.predict(X_test_tfidf)\n",
        "\n",
        "acc = accuracy_score(y_test, y_pred)\n",
        "prec = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "rec = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "fail_count = (y_test != y_pred).sum()\n",
        "fail_percent = (fail_count / len(y_test)) * 100\n",
        "\n",
        "print(\"=\"*30)\n",
        "print(\"EVALUATION METRICS\")\n",
        "print(\"=\"*30)\n",
        "print(f\"Accuracy: {acc:.4f}\")\n",
        "print(f\"Precision: {prec:.4f}\")\n",
        "print(f\"Recall: {rec:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n",
        "print(f\"Failed Predictions: {fail_count} ({fail_percent:.2f}%)\")\n",
        "\n",
        "print(\"\\nClassification Report:\\n\", classification_report(y_test, y_pred, target_names=label_map.keys(), zero_division=0))\n",
        "\n",
        "def predict(text):\n",
        "    cleaned = clean_text(text)\n",
        "    vec = tfidf.transform([cleaned])\n",
        "    pred = mlp.predict(vec)[0]\n",
        "    inv_map = {v:k for k,v in label_map.items()}\n",
        "    return inv_map[pred]\n",
        "\n",
        "print(\"=\"*30)\n",
        "print(\"PREDICTION BOX\")\n",
        "print(\"=\"*30)\n",
        "print(\"Type a sentence to check depression level (or 'exit'):\")\n",
        "\n",
        "while True:\n",
        "    sentence = input(\"\\nEnter text: \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "    if sentence.strip() == \"\":\n",
        "        print(\"Please enter some text.\")\n",
        "        continue\n",
        "\n",
        "    try:\n",
        "        prediction_result = predict(sentence)\n",
        "        print(\"Predicted Category:\", prediction_result)\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during prediction: {e}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YfuyTZIa_7ED",
        "outputId": "b5459fef-9e6e-4830-844c-6dd95cc3a43d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==============================\n",
            "STARTING MLP TRAINING\n",
            "==============================\n",
            "Iteration 1, loss = 1.02239330\n",
            "Validation score: 0.762994\n",
            "Iteration 2, loss = 0.69326283\n",
            "Validation score: 0.844075\n",
            "Iteration 3, loss = 0.36048172\n",
            "Validation score: 0.856549\n",
            "Iteration 4, loss = 0.20263392\n",
            "Validation score: 0.848233\n",
            "Iteration 5, loss = 0.12717117\n",
            "Validation score: 0.856549\n",
            "Iteration 6, loss = 0.07933547\n",
            "Validation score: 0.848233\n",
            "Iteration 7, loss = 0.04953441\n",
            "Validation score: 0.846154\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMAhvvKaqRJmf/M/biMRRiw",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}