{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DasBytes/three-stage-banglish-depression-classifier/blob/main/Banglish_Depression_classifier_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload Code"
      ],
      "metadata": {
        "id": "eoE6tB8EjQWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gelAidcuw6Q2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "8d019a0d-caf3-4552-8a21-8646e0fb6820"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-ecb8d4cd-927d-4637-9bbd-88c4bbef39d9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-ecb8d4cd-927d-4637-9bbd-88c4bbef39d9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Banglish depression dataset.csv to Banglish depression dataset.csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression"
      ],
      "metadata": {
        "id": "T_Hdpoa4jJCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "-__cNI7VxAa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e73ec581-6ed6-4d7c-c9a1-a4c57a96c644"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy : 0.9450343535290443\n",
            "Precision: 0.9457987405657152\n",
            "Recall   : 0.9450467581047381\n",
            "F1-score : 0.945268373230606\n",
            "Enter text for prediction (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.utils import shuffle, resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(file_name, header=None, names=[\"Category\", \"Sentence\"])\n",
        "df.dropna(subset=[\"Sentence\", \"Category\"], inplace=True)\n",
        "df[\"Category\"] = df[\"Category\"].str.strip()\n",
        "df = shuffle(df, random_state=42)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Zআ-হ0-9\\s]\", \" \", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "df[\"Cleaned\"] = df[\"Sentence\"].apply(clean_text)\n",
        "\n",
        "positive_words = ['valo', 'bhalo', 'happy', 'alhamdulillah', 'nice']\n",
        "negative_words = ['kharap', 'na', 'tired', 'stress', 'sad', 'suicide', 'khub']\n",
        "\n",
        "def count_words(text, word_list):\n",
        "    return sum(text.count(w) for w in word_list)\n",
        "\n",
        "df[\"sent_len\"] = df[\"Cleaned\"].apply(lambda x: len(x.split()))\n",
        "df[\"pos_count\"] = df[\"Cleaned\"].apply(lambda x: count_words(x, positive_words))\n",
        "df[\"neg_count\"] = df[\"Cleaned\"].apply(lambda x: count_words(x, negative_words))\n",
        "\n",
        "classes = df[\"Category\"].unique()\n",
        "max_size = df[\"Category\"].value_counts().max()\n",
        "\n",
        "df_balanced = pd.concat([\n",
        "    resample(df[df[\"Category\"] == cls], replace=True, n_samples=max_size, random_state=42)\n",
        "    for cls in classes\n",
        "])\n",
        "\n",
        "df_balanced = shuffle(df_balanced, random_state=42)\n",
        "\n",
        "X_text = df_balanced[\"Cleaned\"]\n",
        "X_num = df_balanced[[\"sent_len\", \"pos_count\", \"neg_count\"]].values\n",
        "y = df_balanced[\"Category\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test, X_train_num, X_test_num = train_test_split(\n",
        "    X_text, y, X_num, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "logistic_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,4))\n",
        "X_train_tfidf = logistic_vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = logistic_vectorizer.transform(X_test_text)\n",
        "\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_num])\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_num])\n",
        "\n",
        "logistic_model = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    multi_class='multinomial',\n",
        "    solver='sag',\n",
        "    C=30,\n",
        "    random_state=42\n",
        ")\n",
        "logistic_model.fit(X_train_combined, y_train)\n",
        "\n",
        "y_pred = logistic_model.predict(X_test_combined)\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred, average=\"macro\"))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred, average=\"macro\"))\n",
        "\n",
        "def predict_live(text):\n",
        "    clean = clean_text(text)\n",
        "    tfidf_vec = logistic_vectorizer.transform([clean])\n",
        "    sent_len = len(clean.split())\n",
        "    pos_count = count_words(clean, positive_words)\n",
        "    neg_count = count_words(clean, negative_words)\n",
        "    num_feat = scaler.transform([[sent_len, pos_count, neg_count]])\n",
        "    combined = hstack([tfidf_vec, num_feat])\n",
        "    pred = model.predict(combined)[0]\n",
        "    prob = np.max(model.predict_proba(combined)) * 100\n",
        "    return pred, prob\n",
        "\n",
        "while True:\n",
        "    txt = input(\"Enter text for prediction (or type 'exit' to quit): \").strip()\n",
        "    if txt.lower() == 'exit':\n",
        "        break\n",
        "    if txt:\n",
        "        pred, conf = predict_live(txt)\n",
        "        print(f\"Prediction: {pred} | Confidence: {conf:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "jY8rOAmki__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install gensim\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "FArW2bCnKOfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "062ff124-2731-47ff-f7e1-afed56921684"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import gensim.downloader as api\n",
        "import nltk\n",
        "nltk.download('stopwords')\n",
        "\n",
        "file_name = \"Banglish depression dataset.csv\"\n",
        "df = pd.read_csv(file_name)\n",
        "df.columns = [\"Category\", \"Sentence\"]\n",
        "df = df.dropna(subset=['Sentence', 'Category'])\n",
        "\n",
        "eng_stop = set(stopwords.words(\"english\"))\n",
        "bn_stop = {\"ami\",\"tumi\",\"amra\",\"valo\",\"kharap\",\"ache\",\"achhi\",\"kintu\",\"na\",\"ar\",\"shob\",\n",
        "           \"ekta\",\"kore\",\"shudhu\",\"amar\",\"tumar\",\"jibone\",\"mone\",\"kotha\",\"ki\",\"kemon\",\n",
        "           \"tome\",\"tomar\",\"tara\",\"tarao\",\"taraor\",\"je\",\"sei\",\"ei\",\"oka\",\"ora\"}\n",
        "stop_words = eng_stop.union(bn_stop)\n",
        "\n",
        "positive_words = {'moja','happy','bhalo','fun','sundor','friend','party','mojar'}\n",
        "negative_words = {'dukho','kharaap','niras','lonely','stress','dukhi','depressed'}\n",
        "\n",
        "tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\u0980-\\u09FF ]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = tokenizer.tokenize(text)\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
        "    return tokens\n",
        "\n",
        "def augment_text(tokens):\n",
        "    if len(tokens) > 1:\n",
        "        idx = np.random.randint(0, len(tokens))\n",
        "        tokens.insert(idx, tokens[idx])\n",
        "    return tokens\n",
        "\n",
        "aug_sentences = []\n",
        "aug_labels = []\n",
        "for sentence, label in zip(df['Sentence'], df['Category']):\n",
        "    tokens = clean_text(sentence)\n",
        "    aug_sentences.append(tokens)\n",
        "    aug_labels.append(label)\n",
        "    aug_sentences.append(augment_text(tokens.copy()))\n",
        "    aug_labels.append(label)\n",
        "\n",
        "df_aug = pd.DataFrame({\"Category\": aug_labels, \"tokens\": aug_sentences})\n",
        "df_aug = shuffle(df_aug, random_state=42)\n",
        "\n",
        "ft_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "embedding_dim = ft_model.vector_size\n",
        "\n",
        "word_index = {word: idx+1 for idx, word in enumerate(ft_model.key_to_index)}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "def tokens_to_sequence(tokens):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "df_aug['seq'] = df_aug['tokens'].apply(tokens_to_sequence)\n",
        "max_len = 50\n",
        "X = pad_sequences(df_aug['seq'], maxlen=max_len)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    try:\n",
        "        embedding_matrix[idx] = ft_model.get_vector(word)\n",
        "    except KeyError:\n",
        "        continue\n",
        "\n",
        "encoder = LabelEncoder()\n",
        "y = encoder.fit_transform(df_aug['Category'])\n",
        "y_cat = to_categorical(y)\n",
        "num_classes = y_cat.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_cat, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim,\n",
        "                    weights=[embedding_matrix], input_length=max_len, trainable=False))\n",
        "lstm_model.add(LSTM(128))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(Dense(64, activation='relu'))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "history = lstm_model.fit(X_train, y_train, epochs=15, batch_size=32, validation_split=0.1, verbose=1)\n",
        "\n",
        "y_pred_probs = lstm_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"✨ lstm_model Evaluation Results ✨\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "def predict_sentence_with_confidence(sentence):\n",
        "    tokens = clean_text(sentence)\n",
        "    if len(set(tokens) & positive_words) > 0:\n",
        "        return \"Non-depression\", 1.0\n",
        "    seq = tokens_to_sequence(tokens)\n",
        "    padded = pad_sequences([seq], maxlen=max_len)\n",
        "    pred = lstm_model.predict(padded)\n",
        "    class_idx = np.argmax(pred)\n",
        "    class_label = encoder.inverse_transform([class_idx])[0]\n",
        "    confidence = pred[0][class_idx]\n",
        "    return class_label, confidence\n",
        "\n",
        "while True:\n",
        "    sentence = input(\"Enter a Banglish sentence (or type 'exit' to quit): \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "    prediction, conf = predict_sentence_with_confidence(sentence)\n",
        "    print(f\"Prediction: {prediction} | Confidence: {conf:.2f}\")\n"
      ],
      "metadata": {
        "id": "ukncfU9gP5iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "40fa043a-b001-4b57-aa85-fb8e887e889f"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 95ms/step - accuracy: 0.5936 - loss: 0.8134 - val_accuracy: 0.7825 - val_loss: 0.4841\n",
            "Epoch 2/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 88ms/step - accuracy: 0.7926 - loss: 0.5079 - val_accuracy: 0.8054 - val_loss: 0.4345\n",
            "Epoch 3/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 101ms/step - accuracy: 0.8157 - loss: 0.4431 - val_accuracy: 0.8345 - val_loss: 0.4204\n",
            "Epoch 4/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - accuracy: 0.8273 - loss: 0.4184 - val_accuracy: 0.8398 - val_loss: 0.3877\n",
            "Epoch 5/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 92ms/step - accuracy: 0.8427 - loss: 0.3837 - val_accuracy: 0.8595 - val_loss: 0.3458\n",
            "Epoch 6/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - accuracy: 0.8585 - loss: 0.3418 - val_accuracy: 0.8595 - val_loss: 0.3391\n",
            "Epoch 7/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 90ms/step - accuracy: 0.8662 - loss: 0.3284 - val_accuracy: 0.8522 - val_loss: 0.3499\n",
            "Epoch 8/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 99ms/step - accuracy: 0.8714 - loss: 0.3136 - val_accuracy: 0.8689 - val_loss: 0.3300\n",
            "Epoch 9/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 93ms/step - accuracy: 0.8959 - loss: 0.2760 - val_accuracy: 0.8918 - val_loss: 0.2950\n",
            "Epoch 10/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 92ms/step - accuracy: 0.9081 - loss: 0.2392 - val_accuracy: 0.8762 - val_loss: 0.3001\n",
            "Epoch 11/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 91ms/step - accuracy: 0.9033 - loss: 0.2417 - val_accuracy: 0.8814 - val_loss: 0.3208\n",
            "Epoch 12/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 91ms/step - accuracy: 0.9022 - loss: 0.2366 - val_accuracy: 0.8824 - val_loss: 0.2870\n",
            "Epoch 13/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 94ms/step - accuracy: 0.9263 - loss: 0.1923 - val_accuracy: 0.8991 - val_loss: 0.2794\n",
            "Epoch 14/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m25s\u001b[0m 92ms/step - accuracy: 0.9348 - loss: 0.1749 - val_accuracy: 0.8980 - val_loss: 0.2721\n",
            "Epoch 15/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m24s\u001b[0m 90ms/step - accuracy: 0.9465 - loss: 0.1390 - val_accuracy: 0.9063 - val_loss: 0.3096\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 33ms/step\n",
            "✨ Model Evaluation Results ✨\n",
            "Accuracy:  0.9005\n",
            "Precision: 0.9008\n",
            "Recall:    0.9005\n",
            "F1 Score:  0.9000\n",
            "Enter a Banglish sentence (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANN MLP"
      ],
      "metadata": {
        "id": "McpT7yXZlJaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "import gensim.downloader as api\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "\n",
        "df = pd.read_csv(\"Banglish depression dataset.csv\")\n",
        "df.dropna(subset=['Sentence', 'Category'], inplace=True)\n",
        "\n",
        "stopwords_eng = set(stopwords.words('english'))\n",
        "stopwords_bangla = {'ami','tumi','shei','amra','eto','kemon','achho','aschi','na'}\n",
        "all_stopwords = stopwords_eng.union(stopwords_bangla)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    tokens = text.split()\n",
        "    tokens = [w for w in tokens if w not in all_stopwords]\n",
        "    return tokens\n",
        "\n",
        "df['Tokens'] = df['Sentence'].apply(clean_text)\n",
        "\n",
        "le = LabelEncoder()\n",
        "df['Label'] = le.fit_transform(df['Category'])\n",
        "num_classes = len(le.classes_)\n",
        "y = to_categorical(df['Label'], num_classes=num_classes)\n",
        "\n",
        "ft_model = api.load('fasttext-wiki-news-subwords-300')\n",
        "embedding_dim = ft_model.vector_size\n",
        "\n",
        "def sentence_to_vec(tokens, model, dim):\n",
        "    vecs = []\n",
        "    for word in tokens:\n",
        "        if word in model:\n",
        "            vecs.append(model[word])\n",
        "    if len(vecs) > 0:\n",
        "        return np.mean(vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "X = np.array([sentence_to_vec(tokens, ft_model, embedding_dim) for tokens in df['Tokens']])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=embedding_dim, activation='relu'))\n",
        "model.add(Dropout(0.3))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.2))\n",
        "model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer=Adam(learning_rate=0.001), loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X_train, y_train, validation_split=0.1, epochs=30, batch_size=32, verbose=2)\n",
        "\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\n✨ Model Evaluation Results ✨\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "def predict_depression(text):\n",
        "    tokens = clean_text(text)\n",
        "    vec = sentence_to_vec(tokens, ft_model, embedding_dim).reshape(1, -1)\n",
        "    pred = model.predict(vec)\n",
        "    pred_class = np.argmax(pred, axis=1)[0]\n",
        "    confidence = np.max(pred)\n",
        "    return le.inverse_transform([pred_class])[0], confidence\n",
        "\n",
        "while True:\n",
        "    sentence = input(\"\\nEnter text (or 'exit'): \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "    category, conf = predict_depression(sentence)\n",
        "    print(f\"Predicted Category: {category} | Confidence: {conf:.2f}\")\n"
      ],
      "metadata": {
        "id": "q0Vyv54qlwHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b904437-3500-4962-8a29-96fb56b263b8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "136/136 - 2s - 15ms/step - accuracy: 0.5869 - loss: 0.8556 - val_accuracy: 0.7131 - val_loss: 0.6974\n",
            "Epoch 2/30\n",
            "136/136 - 1s - 8ms/step - accuracy: 0.6906 - loss: 0.6538 - val_accuracy: 0.7484 - val_loss: 0.6025\n",
            "Epoch 3/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.7489 - loss: 0.5721 - val_accuracy: 0.7651 - val_loss: 0.5502\n",
            "Epoch 4/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.7838 - loss: 0.5176 - val_accuracy: 0.7380 - val_loss: 0.5808\n",
            "Epoch 5/30\n",
            "136/136 - 1s - 8ms/step - accuracy: 0.7887 - loss: 0.4927 - val_accuracy: 0.7630 - val_loss: 0.5392\n",
            "Epoch 6/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.7991 - loss: 0.4857 - val_accuracy: 0.7900 - val_loss: 0.5064\n",
            "Epoch 7/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8079 - loss: 0.4631 - val_accuracy: 0.7879 - val_loss: 0.5048\n",
            "Epoch 8/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8144 - loss: 0.4493 - val_accuracy: 0.7879 - val_loss: 0.5012\n",
            "Epoch 9/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8174 - loss: 0.4393 - val_accuracy: 0.7734 - val_loss: 0.5073\n",
            "Epoch 10/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8195 - loss: 0.4277 - val_accuracy: 0.7817 - val_loss: 0.5194\n",
            "Epoch 11/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8250 - loss: 0.4220 - val_accuracy: 0.7817 - val_loss: 0.5029\n",
            "Epoch 12/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8299 - loss: 0.4135 - val_accuracy: 0.7963 - val_loss: 0.4876\n",
            "Epoch 13/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.8301 - loss: 0.4032 - val_accuracy: 0.7942 - val_loss: 0.4923\n",
            "Epoch 14/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8368 - loss: 0.3936 - val_accuracy: 0.7963 - val_loss: 0.4921\n",
            "Epoch 15/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.8301 - loss: 0.3959 - val_accuracy: 0.8025 - val_loss: 0.4829\n",
            "Epoch 16/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8431 - loss: 0.3852 - val_accuracy: 0.7921 - val_loss: 0.4896\n",
            "Epoch 17/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.8405 - loss: 0.3859 - val_accuracy: 0.7838 - val_loss: 0.4988\n",
            "Epoch 18/30\n",
            "136/136 - 1s - 4ms/step - accuracy: 0.8505 - loss: 0.3682 - val_accuracy: 0.7900 - val_loss: 0.5313\n",
            "Epoch 19/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.8528 - loss: 0.3623 - val_accuracy: 0.8025 - val_loss: 0.4829\n",
            "Epoch 20/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8535 - loss: 0.3515 - val_accuracy: 0.7983 - val_loss: 0.5026\n",
            "Epoch 21/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8519 - loss: 0.3626 - val_accuracy: 0.7900 - val_loss: 0.4948\n",
            "Epoch 22/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8440 - loss: 0.3703 - val_accuracy: 0.7942 - val_loss: 0.4916\n",
            "Epoch 23/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.8632 - loss: 0.3420 - val_accuracy: 0.7859 - val_loss: 0.5102\n",
            "Epoch 24/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8646 - loss: 0.3296 - val_accuracy: 0.7921 - val_loss: 0.5397\n",
            "Epoch 25/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8651 - loss: 0.3260 - val_accuracy: 0.7983 - val_loss: 0.5227\n",
            "Epoch 26/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.8692 - loss: 0.3271 - val_accuracy: 0.8004 - val_loss: 0.5056\n",
            "Epoch 27/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.8771 - loss: 0.3114 - val_accuracy: 0.7921 - val_loss: 0.5145\n",
            "Epoch 28/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.8718 - loss: 0.3106 - val_accuracy: 0.7900 - val_loss: 0.5178\n",
            "Epoch 29/30\n",
            "136/136 - 1s - 8ms/step - accuracy: 0.8822 - loss: 0.2981 - val_accuracy: 0.7921 - val_loss: 0.5082\n",
            "Epoch 30/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.8787 - loss: 0.2993 - val_accuracy: 0.7900 - val_loss: 0.5276\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "✨ Model Evaluation Results ✨\n",
            "Accuracy:  0.8143\n",
            "Precision: 0.8124\n",
            "Recall:    0.8143\n",
            "F1 Score:  0.8131\n",
            "\n",
            "Enter text (or 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random forest\n"
      ],
      "metadata": {
        "id": "Hg8Y3wwOmKjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "file_path = 'Banglish depression dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.columns = [\"Category\", \"Sentence\"]\n",
        "df = df.dropna(subset=['Sentence', 'Category'])\n",
        "\n",
        "eng_stop = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"}\n",
        "bn_stop = {\"ami\",\"tumi\",\"amra\",\"ache\",\"achhi\",\"kintu\",\"na\",\"ar\",\"shob\",\"ekta\",\"kore\",\"shudhu\",\"amar\",\"tumar\",\"mone\",\"kotha\",\"ki\",\"kemon\",\"tome\",\"tomar\",\"tara\",\"tarao\",\"taraor\",\"je\",\"sei\",\"ei\",\"oka\",\"ora\"}\n",
        "stop_words = eng_stop.union(bn_stop)\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = emoji.demojize(text, delimiters=(\" \",\" \"))\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\u0980-\\u09FF ]+\", \" \", text)\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if t not in stop_words and len(t) > 1]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['Cleaned_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "\n",
        "positive_words = {'moja','happy','bhalo','fun','sundor','friend','party','mojar'}\n",
        "negative_words = {'dukho','kharaap','niras','lonely','stress','dukhi','depressed','chinta','udasin'}\n",
        "\n",
        "def augment_text(text):\n",
        "    tokens = text.split()\n",
        "    new_tokens = tokens.copy()\n",
        "    if len(tokens) > 1:\n",
        "        idx = np.random.randint(0, len(tokens))\n",
        "        new_tokens.insert(idx, tokens[idx])\n",
        "    return \" \".join(new_tokens)\n",
        "\n",
        "aug_sentences = []\n",
        "aug_labels = []\n",
        "for sentence, label in zip(df['Cleaned_Sentence'], df['Category']):\n",
        "    aug_sentences.append(sentence)\n",
        "    aug_labels.append(label)\n",
        "    for _ in range(1):\n",
        "        aug_sentences.append(augment_text(sentence))\n",
        "        aug_labels.append(label)\n",
        "\n",
        "df_aug = pd.DataFrame({\"Category\": aug_labels, \"Cleaned_Sentence\": aug_sentences})\n",
        "df_aug = shuffle(df_aug, random_state=42)\n",
        "\n",
        "vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "X = vectorizer.fit_transform(df_aug['Cleaned_Sentence'])\n",
        "y = df_aug['Category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "rf_model = RandomForestClassifier(n_estimators=200, random_state=42, class_weight='balanced')\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "def predict_text(text):\n",
        "    cleaned = preprocess_text(text)\n",
        "    vec = vectorizer.transform([cleaned])\n",
        "    pred = rf_model.predict(vec)[0]\n",
        "    pred_prob = max(rf_model.predict_proba(vec)[0])\n",
        "    tokens = set(cleaned.split())\n",
        "    if len(tokens & positive_words) > 0:\n",
        "        pred = 'Non-depression'\n",
        "    if len(tokens & negative_words) > 0 and pred != 'Non-depression':\n",
        "        pred_prob = min(1.0, pred_prob + 0.1)\n",
        "    return pred, pred_prob\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"Enter a Banglish sentence (or type 'exit' to quit): \")\n",
        "    if text_input.lower() == 'exit':\n",
        "        break\n",
        "    if text_input.strip() == \"\":\n",
        "        continue\n",
        "    prediction, confidence = predict_text(text_input)\n",
        "    print(f\"Predicted Category: {prediction}\")\n",
        "    print(f\"Confidence Score:   {confidence:.2f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "2JDj6pDfmQle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4652c098-62d7-4f20-d217-ec5b03befa74"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9629\n",
            "Precision: 0.9631\n",
            "Recall:    0.9629\n",
            "F1 Score:  0.9629\n",
            "Enter a Banglish sentence (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the trained model"
      ],
      "metadata": {
        "id": "CfCjcmvqppNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n"
      ],
      "metadata": {
        "id": "b36LTnT2fb7L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Precision\": precision,\n",
        "    \"Recall\": recall,\n",
        "    \"F1 Score\": f1\n",
        "}\n",
        "\n",
        "with open(\"banglish_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((rf_model, vectorizer, metrics), f)\n",
        "\n"
      ],
      "metadata": {
        "id": "WSGCn3UXCI8U"
      },
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the trained model"
      ],
      "metadata": {
        "id": "W415CA6jEKrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade streamlit blinker --ignore-installed\n"
      ],
      "metadata": {
        "id": "IO7wS_3dJgdV",
        "outputId": "71849a57-32cb-4b5e-e6c4-5cbbdaa8174e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Using cached streamlit-1.53.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting blinker\n",
            "  Using cached blinker-1.9.0-py3-none-any.whl.metadata (1.6 kB)\n",
            "Collecting altair!=5.4.0,!=5.4.1,<7,>=4.0 (from streamlit)\n",
            "  Downloading altair-6.0.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting cachetools<7,>=5.5 (from streamlit)\n",
            "  Downloading cachetools-6.2.4-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting click<9,>=7.0 (from streamlit)\n",
            "  Downloading click-8.3.1-py3-none-any.whl.metadata (2.6 kB)\n",
            "Collecting numpy<3,>=1.23 (from streamlit)\n",
            "  Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (6.6 kB)\n",
            "Collecting packaging>=20 (from streamlit)\n",
            "  Downloading packaging-25.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting pandas<3,>=1.4.0 (from streamlit)\n",
            "  Downloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m91.2/91.2 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pillow<13,>=7.1.0 (from streamlit)\n",
            "  Downloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (8.8 kB)\n",
            "Collecting protobuf<7,>=3.20 (from streamlit)\n",
            "  Downloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl.metadata (593 bytes)\n",
            "Collecting pyarrow>=7.0 (from streamlit)\n",
            "  Downloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl.metadata (3.2 kB)\n",
            "Collecting requests<3,>=2.27 (from streamlit)\n",
            "  Downloading requests-2.32.5-py3-none-any.whl.metadata (4.9 kB)\n",
            "Collecting tenacity<10,>=8.1.0 (from streamlit)\n",
            "  Downloading tenacity-9.1.2-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting toml<2,>=0.10.1 (from streamlit)\n",
            "  Downloading toml-0.10.2-py2.py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting typing-extensions<5,>=4.10.0 (from streamlit)\n",
            "  Downloading typing_extensions-4.15.0-py3-none-any.whl.metadata (3.3 kB)\n",
            "Collecting watchdog<7,>=2.1.5 (from streamlit)\n",
            "  Using cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl.metadata (44 kB)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Using cached gitpython-3.1.46-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Using cached pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting tornado!=6.5.0,<7,>=6.0.3 (from streamlit)\n",
            "  Downloading tornado-6.5.4-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.8 kB)\n",
            "Collecting jinja2 (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading jinja2-3.1.6-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting jsonschema>=3.0 (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading jsonschema-4.26.0-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting narwhals>=1.27.1 (from altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading narwhals-2.15.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached gitdb-4.0.12-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting python-dateutil>=2.8.2 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting pytz>=2020.1 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Downloading pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
            "Collecting tzdata>=2022.7 (from pandas<3,>=1.4.0->streamlit)\n",
            "  Downloading tzdata-2025.3-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.27->streamlit)\n",
            "  Downloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (37 kB)\n",
            "Collecting idna<4,>=2.5 (from requests<3,>=2.27->streamlit)\n",
            "  Downloading idna-3.11-py3-none-any.whl.metadata (8.4 kB)\n",
            "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.27->streamlit)\n",
            "  Downloading urllib3-2.6.3-py3-none-any.whl.metadata (6.9 kB)\n",
            "Collecting certifi>=2017.4.17 (from requests<3,>=2.27->streamlit)\n",
            "  Downloading certifi-2026.1.4-py3-none-any.whl.metadata (2.5 kB)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Using cached smmap-5.0.2-py3-none-any.whl.metadata (4.3 kB)\n",
            "Collecting MarkupSafe>=2.0 (from jinja2->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl.metadata (2.7 kB)\n",
            "Collecting attrs>=22.2.0 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading attrs-25.4.0-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting jsonschema-specifications>=2023.03.6 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading jsonschema_specifications-2025.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting referencing>=0.28.4 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading referencing-0.37.0-py3-none-any.whl.metadata (2.8 kB)\n",
            "Collecting rpds-py>=0.25.0 (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<7,>=4.0->streamlit)\n",
            "  Downloading rpds_py-0.30.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit)\n",
            "  Downloading six-1.17.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Using cached streamlit-1.53.0-py3-none-any.whl (9.1 MB)\n",
            "Using cached blinker-1.9.0-py3-none-any.whl (8.5 kB)\n",
            "Downloading altair-6.0.0-py3-none-any.whl (795 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m795.4/795.4 kB\u001b[0m \u001b[31m22.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cachetools-6.2.4-py3-none-any.whl (11 kB)\n",
            "Downloading click-8.3.1-py3-none-any.whl (108 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m108.3/108.3 kB\u001b[0m \u001b[31m8.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached gitpython-3.1.46-py3-none-any.whl (208 kB)\n",
            "Downloading numpy-2.4.1-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m168.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading packaging-25.0-py3-none-any.whl (66 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.3-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m131.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pillow-12.1.0-cp312-cp312-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (7.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.0/7.0 MB\u001b[0m \u001b[31m182.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading protobuf-6.33.4-cp39-abi3-manylinux2014_x86_64.whl (323 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m323.3/323.3 kB\u001b[0m \u001b[31m28.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pyarrow-22.0.0-cp312-cp312-manylinux_2_28_x86_64.whl (47.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "Downloading requests-2.32.5-py3-none-any.whl (64 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.7/64.7 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-9.1.2-py3-none-any.whl (28 kB)\n",
            "Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
            "Downloading tornado-6.5.4-cp39-abi3-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (445 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m445.3/445.3 kB\u001b[0m \u001b[31m24.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading typing_extensions-4.15.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached watchdog-6.0.0-py3-none-manylinux2014_x86_64.whl (79 kB)\n",
            "Downloading certifi-2026.1.4-py3-none-any.whl (152 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m152.9/152.9 kB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading charset_normalizer-3.4.4-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (153 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m153.5/153.5 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hUsing cached gitdb-4.0.12-py3-none-any.whl (62 kB)\n",
            "Downloading idna-3.11-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.0/71.0 kB\u001b[0m \u001b[31m7.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jinja2-3.1.6-py3-none-any.whl (134 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.9/134.9 kB\u001b[0m \u001b[31m13.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema-4.26.0-py3-none-any.whl (90 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.6/90.6 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading narwhals-2.15.0-py3-none-any.whl (432 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m432.9/432.9 kB\u001b[0m \u001b[31m31.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m229.9/229.9 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m509.2/509.2 kB\u001b[0m \u001b[31m41.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tzdata-2025.3-py2.py3-none-any.whl (348 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m348.5/348.5 kB\u001b[0m \u001b[31m25.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading urllib3-2.6.3-py3-none-any.whl (131 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.6/131.6 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading attrs-25.4.0-py3-none-any.whl (67 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.6/67.6 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonschema_specifications-2025.9.1-py3-none-any.whl (18 kB)\n",
            "Downloading markupsafe-3.0.3-cp312-cp312-manylinux2014_x86_64.manylinux_2_17_x86_64.manylinux_2_28_x86_64.whl (22 kB)\n",
            "Downloading referencing-0.37.0-py3-none-any.whl (26 kB)\n",
            "Downloading rpds_py-0.30.0-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (394 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m394.1/394.1 kB\u001b[0m \u001b[31m32.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading six-1.17.0-py2.py3-none-any.whl (11 kB)\n",
            "Using cached smmap-5.0.2-py3-none-any.whl (24 kB)\n",
            "Installing collected packages: pytz, watchdog, urllib3, tzdata, typing-extensions, tornado, toml, tenacity, smmap, six, rpds-py, pyarrow, protobuf, pillow, packaging, numpy, narwhals, MarkupSafe, idna, click, charset_normalizer, certifi, cachetools, blinker, attrs, requests, referencing, python-dateutil, jinja2, gitdb, pydeck, pandas, jsonschema-specifications, gitpython, jsonschema, altair, streamlit\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.32.4, but you have requests 2.32.5 which is incompatible.\n",
            "google-colab 1.0.0 requires tornado==6.5.1, but you have tornado 6.5.4 which is incompatible.\n",
            "numba 0.63.1 requires numpy<2.4,>=1.22, but you have numpy 2.4.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed MarkupSafe-3.0.3 altair-6.0.0 attrs-25.4.0 blinker-1.9.0 cachetools-6.2.4 certifi-2026.1.4 charset_normalizer-3.4.4 click-8.3.1 gitdb-4.0.12 gitpython-3.1.46 idna-3.11 jinja2-3.1.6 jsonschema-4.26.0 jsonschema-specifications-2025.9.1 narwhals-2.15.0 numpy-2.4.1 packaging-25.0 pandas-2.3.3 pillow-12.1.0 protobuf-6.33.4 pyarrow-22.0.0 pydeck-0.9.1 python-dateutil-2.9.0.post0 pytz-2025.2 referencing-0.37.0 requests-2.32.5 rpds-py-0.30.0 six-1.17.0 smmap-5.0.2 streamlit-1.53.0 tenacity-9.1.2 toml-0.10.2 tornado-6.5.4 typing-extensions-4.15.0 tzdata-2025.3 urllib3-2.6.3 watchdog-6.0.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "PIL",
                  "certifi",
                  "dateutil",
                  "google",
                  "numpy",
                  "packaging",
                  "pandas",
                  "pyarrow",
                  "pytz",
                  "six",
                  "tornado"
                ]
              },
              "id": "82e4aa95ad224580a354a11c9cab2626"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "st.set_page_config(page_title=\"Banglish Depression Detector\", layout=\"wide\")\n",
        "st.title(\"🧠 Banglish Depression Detection (ML)\")\n",
        "\n",
        "# ------------------- PREPROCESS FUNCTION -------------------\n",
        "def preprocess_text(text):\n",
        "    eng_stop = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\n",
        "                \"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\n",
        "                \"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\n",
        "                \"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\n",
        "                \"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\n",
        "                \"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\n",
        "                \"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\n",
        "                \"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\n",
        "                \"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\n",
        "                \"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\n",
        "                \"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\n",
        "                \"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"}\n",
        "\n",
        "    bn_stop = {\"ami\",\"tumi\",\"amra\",\"ache\",\"achhi\",\"kintu\",\"na\",\"ar\",\"shob\",\"ekta\",\n",
        "               \"kore\",\"shudhu\",\"amar\",\"tumar\",\"mone\",\"kotha\",\"ki\",\"kemon\",\n",
        "               \"tome\",\"tomar\",\"tara\",\"tarao\",\"taraor\",\"je\",\"sei\",\"ei\",\"oka\",\"ora\"}\n",
        "\n",
        "    stop_words = eng_stop.union(bn_stop)\n",
        "\n",
        "    text = str(text).lower()\n",
        "    text = emoji.demojize(text, delimiters=(\" \",\" \"))\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\u0980-\\u09FF ]+\", \" \", text)\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = [t for t in text.split() if t not in stop_words and len(t) > 1]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ------------------- LOAD PICKLED MODEL -------------------\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    with open(\"banglish_model.pkl\", \"rb\") as f:\n",
        "        model, vectorizer, metrics = pickle.load(f)\n",
        "    return model, vectorizer, metrics\n",
        "\n",
        "model, vectorizer, metrics = load_model()\n",
        "\n",
        "# ------------------- SIDEBAR -------------------\n",
        "st.sidebar.header(\"Model Performance on Test Set\")\n",
        "for k, v in metrics.items():\n",
        "    st.sidebar.write(f\"**{k}:** {v:.4f}\")\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.info(\"Trained on Banglish dataset with stopwords removed, TF-IDF + Random Forest. Includes augmentation & positive/negative word tweaks.\")\n",
        "\n",
        "# ------------------- MAIN PANEL -------------------\n",
        "st.subheader(\"Enter a Banglish sentence for prediction:\")\n",
        "\n",
        "text_input = st.text_area(\"\")\n",
        "\n",
        "if st.button(\"Predict\") and text_input.strip():\n",
        "    clean = preprocess_text(text_input)\n",
        "    vec = vectorizer.transform([clean])\n",
        "    pred = model.predict(vec)[0]\n",
        "    prob = max(model.predict_proba(vec)[0])\n",
        "\n",
        "    # Positive/Negative words adjustment\n",
        "    positive_words = {'moja','happy','bhalo','fun','sundor','friend','party','mojar'}\n",
        "    negative_words = {'dukho','kharaap','niras','lonely','stress','dukhi','depressed','chinta','udasin'}\n",
        "    tokens = set(clean.split())\n",
        "    if len(tokens & positive_words) > 0:\n",
        "        pred = 'Non-depression'\n",
        "    if len(tokens & negative_words) > 0 and pred != 'Non-depression':\n",
        "        prob = min(1.0, prob + 0.1)\n",
        "\n",
        "    st.success(f\"**Prediction:** {pred}\")\n",
        "    st.info(f\"**Confidence Score:** {prob:.2f}\")\n",
        "\n",
        "    # ------------------- TOP TF-IDF WORDS -------------------\n",
        "    st.subheader(\"Top words contributing (TF-IDF):\")\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "    tfidf_vec = vec.toarray()[0]\n",
        "    top_idx = tfidf_vec.argsort()[-10:][::-1]\n",
        "    top_words = [(feature_names[i], tfidf_vec[i]) for i in top_idx if tfidf_vec[i] > 0]\n",
        "    if top_words:\n",
        "        for word, score in top_words:\n",
        "            st.write(f\"{word}: {score:.4f}\")\n",
        "    else:\n",
        "        st.write(\"No significant words found.\")\n"
      ],
      "metadata": {
        "id": "vlA1XIU9HxPR",
        "outputId": "f8e2f24f-4ded-42d7-b0fa-8ee0f6ddc7da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &>/content/streamlit.log &\n"
      ],
      "metadata": {
        "id": "i-K5F7JRH3vY"
      },
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n"
      ],
      "metadata": {
        "id": "_nvG-JHxKkTD"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cloudflared-linux-amd64 tunnel --url http://localhost:8501\n"
      ],
      "metadata": {
        "id": "u0oznLBjKpNr",
        "outputId": "c9b7c2ce-8c5a-4479-8b52-943e5aaa0c0b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[90m2026-01-15T15:36:47Z\u001b[0m \u001b[32mINF\u001b[0m Thank you for trying Cloudflare Tunnel. Doing so, without a Cloudflare account, is a quick way to experiment and try it out. However, be aware that these account-less Tunnels have no uptime guarantee, are subject to the Cloudflare Online Services Terms of Use (https://www.cloudflare.com/website-terms/), and Cloudflare reserves the right to investigate your use of Tunnels for violations of such terms. If you intend to use Tunnels in production you should use a pre-created named tunnel by following: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps\n",
            "\u001b[90m2026-01-15T15:36:47Z\u001b[0m \u001b[32mINF\u001b[0m Requesting new quick Tunnel on trycloudflare.com...\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m |  Your quick Tunnel has been created! Visit it at (it may take some time to be reachable):  |\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m |  https://incidents-cooperative-informational-pensions.trycloudflare.com                    |\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m +--------------------------------------------------------------------------------------------+\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Cannot determine default configuration path. No file [config.yml config.yaml] in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Version 2025.11.1 (Checksum 991dffd8889ee9f0147b6b48933da9e4407e68ea8c6d984f55fa2d3db4bb431d)\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m GOOS: linux, GOVersion: go1.24.9, GoArch: amd64\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Settings: map[ha-connections:1 protocol:quic url:http://localhost:8501]\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m cloudflared will not automatically update when run from the shell. To enable auto-updates, run cloudflared as a service: https://developers.cloudflare.com/cloudflare-one/connections/connect-apps/configure-tunnels/local-management/as-a-service/\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Generated Connector ID: 643db1b7-ea50-4cf1-b7f7-e98916209dcc\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Initial protocol quic\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Cannot determine default origin certificate path. No file cert.pem in [~/.cloudflared ~/.cloudflare-warp ~/cloudflare-warp /etc/cloudflared /usr/local/etc/cloudflared]. You need to specify the origin certificate path by specifying the origincert option in the configuration file, or set TUNNEL_ORIGIN_CERT environment variable \u001b[36moriginCertPath=\u001b[0m\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use 172.28.0.12 as source for IPv4\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m ICMP proxy will use ::1 in zone lo as source for IPv6\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Starting metrics server on 127.0.0.1:20241/metrics\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel connection curve preferences: [X25519MLKEM768 CurveP256] \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47\n",
            "2026/01/15 15:36:51 failed to sufficiently increase receive buffer size (was: 208 kiB, wanted: 7168 kiB, got: 416 kiB). See https://github.com/quic-go/quic-go/wiki/UDP-Buffer-Sizes for details.\n",
            "\u001b[90m2026-01-15T15:36:51Z\u001b[0m \u001b[32mINF\u001b[0m Registered tunnel connection \u001b[36mconnIndex=\u001b[0m0 \u001b[36mconnection=\u001b[0m6a18b3b2-650a-49fc-88ea-80c8f3dea932 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47 \u001b[36mlocation=\u001b[0msea01 \u001b[36mprotocol=\u001b[0mquic\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[32mINF\u001b[0m Initiating graceful shutdown due to signal interrupt ...\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to run the datagram handler \u001b[31merror=\u001b[0m\u001b[31m\"context canceled\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m failed to serve tunnel connection \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Serve tunnel error \u001b[31merror=\u001b[0m\u001b[31m\"accept stream listener encountered a failure while serving\"\u001b[0m \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[32mINF\u001b[0m Retrying connection in up to 1s \u001b[36mconnIndex=\u001b[0m0 \u001b[36mevent=\u001b[0m0 \u001b[36mip=\u001b[0m198.41.192.47\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m Connection terminated \u001b[36mconnIndex=\u001b[0m0\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[1m\u001b[31mERR\u001b[0m\u001b[0m no more connections active and exiting\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[32mINF\u001b[0m Tunnel server stopped\n",
            "\u001b[90m2026-01-15T15:40:14Z\u001b[0m \u001b[32mINF\u001b[0m Metrics server stopped\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "save and download model"
      ],
      "metadata": {
        "id": "HqCR5kncco9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "with open(\"logistic_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((model, vectorizer, scaler), f)\n",
        "\n",
        "# --- Random Forest ---\n",
        "with open(\"rf_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((rf_model, vectorizer, stop_words), f)\n"
      ],
      "metadata": {
        "id": "Hi8eyGQRcmGx"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LSTM ---\n",
        "lstm_model.save(\"lstm_model.h5\")\n",
        "\n",
        "# --- ANN/MLP ---\n",
        "mlp_model.save(\"mlp_model.h5\")\n"
      ],
      "metadata": {
        "id": "qJp0x4Ihk162",
        "outputId": "e7417f0b-736c-4842-bf7d-e4bc440189b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        }
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'lstm_model' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3888795697.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- LSTM ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"lstm_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# --- ANN/MLP ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mmlp_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"mlp_model.h5\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'lstm_model' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}