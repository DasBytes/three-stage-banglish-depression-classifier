{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DasBytes/three-stage-banglish-depression-classifier/blob/main/Banglish_Depression_classifier_Final.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Upload Code"
      ],
      "metadata": {
        "id": "eoE6tB8EjQWn"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "gelAidcuw6Q2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "f900d8c3-3e05-4bd7-9caf-d6663625cb21"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-e2b59dae-64d0-4def-9cff-7d8c4c080ff5\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-e2b59dae-64d0-4def-9cff-7d8c4c080ff5\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Banglish depression dataset.csv to Banglish depression dataset (1).csv\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Logistic regression"
      ],
      "metadata": {
        "id": "T_Hdpoa4jJCB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "-__cNI7VxAa6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "81ad4ed3-c4ee-40c2-b648-4787b0d1ee0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy : 0.9450343535290443\n",
            "Precision: 0.9457987405657152\n",
            "Recall   : 0.9450467581047381\n",
            "F1-score : 0.945268373230606\n",
            "Enter text for prediction (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "from sklearn.utils import shuffle, resample\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from scipy.sparse import hstack\n",
        "\n",
        "file_name = list(uploaded.keys())[0]\n",
        "df = pd.read_csv(file_name, header=None, names=[\"Category\", \"Sentence\"])\n",
        "df.dropna(subset=[\"Sentence\", \"Category\"], inplace=True)\n",
        "df[\"Category\"] = df[\"Category\"].str.strip()\n",
        "df = shuffle(df, random_state=42)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Zআ-হ0-9\\s]\", \" \", text)\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "df[\"Cleaned\"] = df[\"Sentence\"].apply(clean_text)\n",
        "\n",
        "positive_words = ['valo', 'bhalo', 'happy', 'alhamdulillah', 'nice']\n",
        "negative_words = ['kharap', 'na', 'tired', 'stress', 'sad', 'suicide', 'khub']\n",
        "\n",
        "def count_words(text, word_list):\n",
        "    return sum(text.count(w) for w in word_list)\n",
        "\n",
        "df[\"sent_len\"] = df[\"Cleaned\"].apply(lambda x: len(x.split()))\n",
        "df[\"pos_count\"] = df[\"Cleaned\"].apply(lambda x: count_words(x, positive_words))\n",
        "df[\"neg_count\"] = df[\"Cleaned\"].apply(lambda x: count_words(x, negative_words))\n",
        "\n",
        "classes = df[\"Category\"].unique()\n",
        "max_size = df[\"Category\"].value_counts().max()\n",
        "\n",
        "df_balanced = pd.concat([\n",
        "    resample(df[df[\"Category\"] == cls], replace=True, n_samples=max_size, random_state=42)\n",
        "    for cls in classes\n",
        "])\n",
        "\n",
        "df_balanced = shuffle(df_balanced, random_state=42)\n",
        "\n",
        "X_text = df_balanced[\"Cleaned\"]\n",
        "X_num = df_balanced[[\"sent_len\", \"pos_count\", \"neg_count\"]].values\n",
        "y = df_balanced[\"Category\"]\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_num = scaler.fit_transform(X_num)\n",
        "\n",
        "X_train_text, X_test_text, y_train, y_test, X_train_num, X_test_num = train_test_split(\n",
        "    X_text, y, X_num, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "logistic_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,4))\n",
        "X_train_tfidf = logistic_vectorizer.fit_transform(X_train_text)\n",
        "X_test_tfidf = logistic_vectorizer.transform(X_test_text)\n",
        "\n",
        "X_train_combined = hstack([X_train_tfidf, X_train_num])\n",
        "X_test_combined = hstack([X_test_tfidf, X_test_num])\n",
        "\n",
        "logistic_model = LogisticRegression(\n",
        "    max_iter=2000,\n",
        "    multi_class='multinomial',\n",
        "    solver='sag',\n",
        "    C=30,\n",
        "    random_state=42\n",
        ")\n",
        "logistic_model.fit(X_train_combined, y_train)\n",
        "\n",
        "y_pred = logistic_model.predict(X_test_combined)\n",
        "print(\"Accuracy :\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred, average=\"macro\"))\n",
        "print(\"Recall   :\", recall_score(y_test, y_pred, average=\"macro\"))\n",
        "print(\"F1-score :\", f1_score(y_test, y_pred, average=\"macro\"))\n",
        "\n",
        "def predict_live(text):\n",
        "    clean = clean_text(text)\n",
        "    tfidf_vec = logistic_vectorizer.transform([clean])\n",
        "    sent_len = len(clean.split())\n",
        "    pos_count = count_words(clean, positive_words)\n",
        "    neg_count = count_words(clean, negative_words)\n",
        "    num_feat = scaler.transform([[sent_len, pos_count, neg_count]])\n",
        "    combined = hstack([tfidf_vec, num_feat])\n",
        "    pred = model.predict(combined)[0]\n",
        "    prob = np.max(model.predict_proba(combined)) * 100\n",
        "    return pred, prob\n",
        "\n",
        "while True:\n",
        "    txt = input(\"Enter text for prediction (or type 'exit' to quit): \").strip()\n",
        "    if txt.lower() == 'exit':\n",
        "        break\n",
        "    if txt:\n",
        "        pred, conf = predict_live(txt)\n",
        "        print(f\"Prediction: {pred} | Confidence: {conf:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM"
      ],
      "metadata": {
        "id": "jY8rOAmki__Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install emoji\n",
        "!pip install gensim\n",
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "FArW2bCnKOfs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99f9867-fcb7-4c9c-be85-6e40ea14bb1c"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting emoji\n",
            "  Downloading emoji-2.15.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Downloading emoji-2.15.0-py3-none-any.whl (608 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/608.4 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m608.4/608.4 kB\u001b[0m \u001b[31m21.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: emoji\n",
            "Successfully installed emoji-2.15.0\n",
            "Collecting gensim\n",
            "  Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl.metadata (8.4 kB)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.12/dist-packages (from gensim) (2.0.2)\n",
            "Requirement already satisfied: scipy>=1.7.0 in /usr/local/lib/python3.12/dist-packages (from gensim) (1.16.3)\n",
            "Requirement already satisfied: smart_open>=1.8.1 in /usr/local/lib/python3.12/dist-packages (from gensim) (7.5.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.12/dist-packages (from smart_open>=1.8.1->gensim) (2.0.1)\n",
            "Downloading gensim-4.4.0-cp312-cp312-manylinux_2_24_x86_64.manylinux_2_28_x86_64.whl (27.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.9/27.9 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: gensim\n",
            "Successfully installed gensim-4.4.0\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.12.19)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.7.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.1)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.76.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.15.1)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.4)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.18.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2026.1.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.5)\n",
            "Requirement already satisfied: markupsafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense, Embedding, Dropout\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import gensim.downloader as api\n",
        "import nltk\n",
        "\n",
        "file_name = \"Banglish depression dataset.csv\"\n",
        "df = pd.read_csv(file_name)\n",
        "df.columns = [\"Category\", \"Sentence\"]\n",
        "df = df.dropna(subset=['Sentence', 'Category'])\n",
        "\n",
        "positive_words = {'moja','happy','bhalo','fun','sundor','friend','party','mojar'}\n",
        "negative_words = {'dukho','kharaap','niras','lonely','stress','dukhi','depressed'}\n",
        "\n",
        "lstm_tokenizer = RegexpTokenizer(r'\\w+')\n",
        "\n",
        "def clean_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = emoji.replace_emoji(text, replace='')\n",
        "    text = re.sub(r\"http\\S+|www\\S+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\u0980-\\u09FF ]+\", \" \", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = lstm_tokenizer.tokenize(text)\n",
        "    tokens = [t for t in tokens if len(t) > 1]\n",
        "    return tokens\n",
        "\n",
        "def augment_text(tokens):\n",
        "    if len(tokens) > 1:\n",
        "        idx = np.random.randint(0, len(tokens))\n",
        "        tokens.insert(idx, tokens[idx])\n",
        "    return tokens\n",
        "\n",
        "aug_sentences = []\n",
        "aug_labels = []\n",
        "\n",
        "for sentence, label in zip(df['Sentence'], df['Category']):\n",
        "    tokens = clean_text(sentence)\n",
        "    aug_sentences.append(tokens)\n",
        "    aug_labels.append(label)\n",
        "\n",
        "    aug_sentences.append(augment_text(tokens.copy()))\n",
        "    aug_labels.append(label)\n",
        "\n",
        "df_aug = pd.DataFrame({\"Category\": aug_labels, \"tokens\": aug_sentences})\n",
        "df_aug = shuffle(df_aug, random_state=42)\n",
        "\n",
        "ft_model = api.load(\"fasttext-wiki-news-subwords-300\")\n",
        "embedding_dim = ft_model.vector_size\n",
        "\n",
        "word_index = {word: idx + 1 for idx, word in enumerate(ft_model.key_to_index)}\n",
        "vocab_size = len(word_index) + 1\n",
        "\n",
        "def tokens_to_sequence(tokens):\n",
        "    return [word_index[t] for t in tokens if t in word_index]\n",
        "\n",
        "df_aug['seq'] = df_aug['tokens'].apply(tokens_to_sequence)\n",
        "\n",
        "max_len = 50\n",
        "X = pad_sequences(df_aug['seq'], maxlen=max_len)\n",
        "\n",
        "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
        "for word, idx in word_index.items():\n",
        "    try:\n",
        "        embedding_matrix[idx] = ft_model.get_vector(word)\n",
        "    except KeyError:\n",
        "        continue\n",
        "\n",
        "lstm_label_encoder = LabelEncoder()\n",
        "y = lstm_label_encoder.fit_transform(df_aug['Category'])\n",
        "y_cat = to_categorical(y)\n",
        "num_classes = y_cat.shape[1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y_cat, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "lstm_model = Sequential()\n",
        "lstm_model.add(\n",
        "    Embedding(\n",
        "        input_dim=vocab_size,\n",
        "        output_dim=embedding_dim,\n",
        "        weights=[embedding_matrix],\n",
        "        input_length=max_len,\n",
        "        trainable=False\n",
        "    )\n",
        ")\n",
        "lstm_model.add(LSTM(128))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(Dense(64, activation='relu'))\n",
        "lstm_model.add(Dropout(0.3))\n",
        "lstm_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "lstm_model.compile(\n",
        "    loss='categorical_crossentropy',\n",
        "    optimizer='adam',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "history = lstm_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    epochs=15,\n",
        "    batch_size=32,\n",
        "    validation_split=0.1,\n",
        "    verbose=1\n",
        ")\n",
        "\n",
        "y_pred_probs = lstm_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred_probs, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"✨ lstm_model Evaluation Results ✨\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "def predict_sentence_with_confidence(sentence):\n",
        "    tokens = clean_text(sentence)\n",
        "\n",
        "    if len(set(tokens) & positive_words) > 0:\n",
        "        return \"Non-depression\", 1.0\n",
        "\n",
        "    seq = tokens_to_sequence(tokens)\n",
        "    padded = pad_sequences([seq], maxlen=max_len)\n",
        "    pred = lstm_model.predict(padded)\n",
        "\n",
        "    class_idx = np.argmax(pred)\n",
        "    class_label = lstm_label_encoder.inverse_transform([class_idx])[0]\n",
        "    confidence = pred[0][class_idx]\n",
        "\n",
        "    return class_label, confidence\n",
        "\n",
        "while True:\n",
        "    sentence = input(\"Enter a Banglish sentence (or type 'exit' to quit): \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "    prediction, conf = predict_sentence_with_confidence(sentence)\n",
        "    print(f\"Prediction: {prediction} | Confidence: {conf:.2f}\")\n"
      ],
      "metadata": {
        "id": "ukncfU9gP5iu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "490143ae-eb7c-491b-a879-c697bd5e5b67"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 958.5/958.4MB downloaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/embedding.py:97: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 100ms/step - accuracy: 0.6148 - loss: 0.7846 - val_accuracy: 0.7856 - val_loss: 0.5034\n",
            "Epoch 2/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 101ms/step - accuracy: 0.7862 - loss: 0.5221 - val_accuracy: 0.8231 - val_loss: 0.4248\n",
            "Epoch 3/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 99ms/step - accuracy: 0.8218 - loss: 0.4429 - val_accuracy: 0.8189 - val_loss: 0.4422\n",
            "Epoch 4/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 99ms/step - accuracy: 0.8214 - loss: 0.4346 - val_accuracy: 0.8241 - val_loss: 0.4295\n",
            "Epoch 5/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 99ms/step - accuracy: 0.8417 - loss: 0.3936 - val_accuracy: 0.8543 - val_loss: 0.3578\n",
            "Epoch 6/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 105ms/step - accuracy: 0.8558 - loss: 0.3579 - val_accuracy: 0.8678 - val_loss: 0.3298\n",
            "Epoch 7/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 100ms/step - accuracy: 0.8625 - loss: 0.3427 - val_accuracy: 0.8595 - val_loss: 0.3370\n",
            "Epoch 8/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 100ms/step - accuracy: 0.8743 - loss: 0.3216 - val_accuracy: 0.8606 - val_loss: 0.3304\n",
            "Epoch 9/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 99ms/step - accuracy: 0.8847 - loss: 0.2915 - val_accuracy: 0.8793 - val_loss: 0.2871\n",
            "Epoch 10/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 102ms/step - accuracy: 0.8941 - loss: 0.2709 - val_accuracy: 0.8918 - val_loss: 0.2814\n",
            "Epoch 11/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 100ms/step - accuracy: 0.9054 - loss: 0.2353 - val_accuracy: 0.8835 - val_loss: 0.2944\n",
            "Epoch 12/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 100ms/step - accuracy: 0.9092 - loss: 0.2279 - val_accuracy: 0.8907 - val_loss: 0.2784\n",
            "Epoch 13/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 100ms/step - accuracy: 0.9317 - loss: 0.1898 - val_accuracy: 0.8939 - val_loss: 0.2728\n",
            "Epoch 14/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 103ms/step - accuracy: 0.9370 - loss: 0.1713 - val_accuracy: 0.8855 - val_loss: 0.2793\n",
            "Epoch 15/15\n",
            "\u001b[1m271/271\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m27s\u001b[0m 100ms/step - accuracy: 0.9420 - loss: 0.1535 - val_accuracy: 0.9053 - val_loss: 0.2340\n",
            "\u001b[1m76/76\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 36ms/step\n",
            "✨ lstm_model Evaluation Results ✨\n",
            "Accuracy:  0.9026\n",
            "Precision: 0.9033\n",
            "Recall:    0.9026\n",
            "F1 Score:  0.9028\n",
            "Enter a Banglish sentence (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#ANN MLP"
      ],
      "metadata": {
        "id": "McpT7yXZlJaN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "import gensim.downloader as api\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import nltk\n",
        "\n",
        "df = pd.read_csv(\"Banglish depression dataset.csv\")\n",
        "df.dropna(subset=['Sentence', 'Category'], inplace=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "    text = text.lower()\n",
        "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
        "    text = re.sub(r'\\S+@\\S+', '', text)\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "    text = emoji.demojize(text, delimiters=(\" \", \" \"))\n",
        "    text = re.sub(r'(.)\\1{2,}', r'\\1\\1', text)\n",
        "    tokens = text.split()\n",
        "    return tokens\n",
        "\n",
        "df['Tokens'] = df['Sentence'].apply(clean_text)\n",
        "\n",
        "mlp_label_encoder = LabelEncoder()\n",
        "df['Label'] = mlp_label_encoder.fit_transform(df['Category'])\n",
        "num_classes = len(mlp_label_encoder.classes_)\n",
        "y = to_categorical(df['Label'], num_classes=num_classes)\n",
        "\n",
        "ft_model = api.load('fasttext-wiki-news-subwords-300')\n",
        "embedding_dim = ft_model.vector_size\n",
        "\n",
        "def sentence_to_vec(tokens, model, dim):\n",
        "    vecs = []\n",
        "    for word in tokens:\n",
        "        if word in model:\n",
        "            vecs.append(model[word])\n",
        "    if len(vecs) > 0:\n",
        "        return np.mean(vecs, axis=0)\n",
        "    else:\n",
        "        return np.zeros(dim)\n",
        "\n",
        "X = np.array([\n",
        "    sentence_to_vec(tokens, ft_model, embedding_dim)\n",
        "    for tokens in df['Tokens']\n",
        "])\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "mlp_model = Sequential()\n",
        "mlp_model.add(Dense(128, input_dim=embedding_dim, activation='relu'))\n",
        "mlp_model.add(Dropout(0.3))\n",
        "mlp_model.add(Dense(64, activation='relu'))\n",
        "mlp_model.add(Dropout(0.2))\n",
        "mlp_model.add(Dense(num_classes, activation='softmax'))\n",
        "\n",
        "mlp_model.compile(\n",
        "    optimizer=Adam(learning_rate=0.001),\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "mlp_model.fit(\n",
        "    X_train,\n",
        "    y_train,\n",
        "    validation_split=0.1,\n",
        "    epochs=30,\n",
        "    batch_size=32,\n",
        "    verbose=2\n",
        ")\n",
        "\n",
        "y_pred = mlp_model.predict(X_test)\n",
        "y_pred_classes = np.argmax(y_pred, axis=1)\n",
        "y_true_classes = np.argmax(y_test, axis=1)\n",
        "\n",
        "accuracy = accuracy_score(y_true_classes, y_pred_classes)\n",
        "precision = precision_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_true_classes, y_pred_classes, average='weighted', zero_division=0)\n",
        "\n",
        "print(\"\\n✨ mlp_model Evaluation Results ✨\")\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "def predict_depression(text):\n",
        "    tokens = clean_text(text)\n",
        "    vec = sentence_to_vec(tokens, ft_model, embedding_dim).reshape(1, -1)\n",
        "    pred = mlp_model.predict(vec)\n",
        "    pred_class = np.argmax(pred, axis=1)[0]\n",
        "    confidence = np.max(pred)\n",
        "    return mlp_label_encoder.inverse_transform([pred_class])[0], confidence\n",
        "\n",
        "while True:\n",
        "    sentence = input(\"\\nEnter text (or 'exit'): \")\n",
        "    if sentence.lower() == 'exit':\n",
        "        break\n",
        "    category, conf = predict_depression(sentence)\n",
        "    print(f\"Predicted Category: {category} | Confidence: {conf:.2f}\")\n"
      ],
      "metadata": {
        "id": "q0Vyv54qlwHH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0c43036-8ebe-4cc3-8225-e7cb7bd4b08f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/30\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "136/136 - 2s - 14ms/step - accuracy: 0.5883 - loss: 0.8213 - val_accuracy: 0.6715 - val_loss: 0.7041\n",
            "Epoch 2/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.6966 - loss: 0.6554 - val_accuracy: 0.7505 - val_loss: 0.6401\n",
            "Epoch 3/30\n",
            "136/136 - 1s - 4ms/step - accuracy: 0.7373 - loss: 0.5811 - val_accuracy: 0.7672 - val_loss: 0.5677\n",
            "Epoch 4/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.7695 - loss: 0.5317 - val_accuracy: 0.7755 - val_loss: 0.5411\n",
            "Epoch 5/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.7818 - loss: 0.5078 - val_accuracy: 0.7796 - val_loss: 0.5328\n",
            "Epoch 6/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.7970 - loss: 0.4792 - val_accuracy: 0.7838 - val_loss: 0.5192\n",
            "Epoch 7/30\n",
            "136/136 - 1s - 4ms/step - accuracy: 0.8007 - loss: 0.4647 - val_accuracy: 0.7921 - val_loss: 0.5026\n",
            "Epoch 8/30\n",
            "136/136 - 1s - 4ms/step - accuracy: 0.7966 - loss: 0.4645 - val_accuracy: 0.7796 - val_loss: 0.5008\n",
            "Epoch 9/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8075 - loss: 0.4503 - val_accuracy: 0.7775 - val_loss: 0.5094\n",
            "Epoch 10/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.8098 - loss: 0.4367 - val_accuracy: 0.7817 - val_loss: 0.5022\n",
            "Epoch 11/30\n",
            "136/136 - 1s - 4ms/step - accuracy: 0.8216 - loss: 0.4217 - val_accuracy: 0.7692 - val_loss: 0.5164\n",
            "Epoch 12/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8227 - loss: 0.4168 - val_accuracy: 0.7796 - val_loss: 0.5232\n",
            "Epoch 13/30\n",
            "136/136 - 1s - 4ms/step - accuracy: 0.8123 - loss: 0.4346 - val_accuracy: 0.7879 - val_loss: 0.5020\n",
            "Epoch 14/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8220 - loss: 0.4170 - val_accuracy: 0.7983 - val_loss: 0.4949\n",
            "Epoch 15/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8285 - loss: 0.4038 - val_accuracy: 0.7963 - val_loss: 0.5294\n",
            "Epoch 16/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8315 - loss: 0.3999 - val_accuracy: 0.8067 - val_loss: 0.4735\n",
            "Epoch 17/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.8318 - loss: 0.3906 - val_accuracy: 0.7942 - val_loss: 0.4950\n",
            "Epoch 18/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8426 - loss: 0.3778 - val_accuracy: 0.8025 - val_loss: 0.4822\n",
            "Epoch 19/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8371 - loss: 0.3830 - val_accuracy: 0.8129 - val_loss: 0.4776\n",
            "Epoch 20/30\n",
            "136/136 - 0s - 4ms/step - accuracy: 0.8419 - loss: 0.3723 - val_accuracy: 0.7942 - val_loss: 0.4831\n",
            "Epoch 21/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8318 - loss: 0.3804 - val_accuracy: 0.8067 - val_loss: 0.4785\n",
            "Epoch 22/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8459 - loss: 0.3682 - val_accuracy: 0.7983 - val_loss: 0.4904\n",
            "Epoch 23/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8454 - loss: 0.3605 - val_accuracy: 0.8087 - val_loss: 0.4935\n",
            "Epoch 24/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8567 - loss: 0.3489 - val_accuracy: 0.8108 - val_loss: 0.4716\n",
            "Epoch 25/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8533 - loss: 0.3485 - val_accuracy: 0.8212 - val_loss: 0.4594\n",
            "Epoch 26/30\n",
            "136/136 - 0s - 3ms/step - accuracy: 0.8574 - loss: 0.3427 - val_accuracy: 0.8150 - val_loss: 0.4736\n",
            "Epoch 27/30\n",
            "136/136 - 1s - 4ms/step - accuracy: 0.8577 - loss: 0.3398 - val_accuracy: 0.8087 - val_loss: 0.4791\n",
            "Epoch 28/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.8591 - loss: 0.3379 - val_accuracy: 0.8025 - val_loss: 0.4808\n",
            "Epoch 29/30\n",
            "136/136 - 1s - 5ms/step - accuracy: 0.8572 - loss: 0.3416 - val_accuracy: 0.8129 - val_loss: 0.4719\n",
            "Epoch 30/30\n",
            "136/136 - 1s - 8ms/step - accuracy: 0.8547 - loss: 0.3389 - val_accuracy: 0.8233 - val_loss: 0.4730\n",
            "\u001b[1m38/38\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step\n",
            "\n",
            "✨ mlp_model Evaluation Results ✨\n",
            "Accuracy:  0.8193\n",
            "Precision: 0.8210\n",
            "Recall:    0.8193\n",
            "F1 Score:  0.8200\n",
            "\n",
            "Enter text (or 'exit'): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Random forest\n"
      ],
      "metadata": {
        "id": "Hg8Y3wwOmKjI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "file_path = 'Banglish depression dataset.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "df.columns = [\"Category\", \"Sentence\"]\n",
        "df = df.dropna(subset=['Sentence', 'Category'])\n",
        "\n",
        "def preprocess_text(text):\n",
        "    text = str(text).lower()\n",
        "    text = emoji.demojize(text, delimiters=(\" \",\" \"))\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\u0980-\\u09FF ]+\", \" \", text)\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = text.split()\n",
        "    tokens = [t for t in tokens if len(t) > 1]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "df['Cleaned_Sentence'] = df['Sentence'].apply(preprocess_text)\n",
        "\n",
        "positive_words = {'moja','happy','bhalo','fun','sundor','friend','party','mojar'}\n",
        "negative_words = {'dukho','kharaap','niras','lonely','stress','dukhi','depressed','chinta','udasin'}\n",
        "\n",
        "def augment_text(text):\n",
        "    tokens = text.split()\n",
        "    new_tokens = tokens.copy()\n",
        "    if len(tokens) > 1:\n",
        "        idx = np.random.randint(0, len(tokens))\n",
        "        new_tokens.insert(idx, tokens[idx])\n",
        "    return \" \".join(new_tokens)\n",
        "\n",
        "aug_sentences = []\n",
        "aug_labels = []\n",
        "\n",
        "for sentence, label in zip(df['Cleaned_Sentence'], df['Category']):\n",
        "    aug_sentences.append(sentence)\n",
        "    aug_labels.append(label)\n",
        "    for _ in range(1):\n",
        "        aug_sentences.append(augment_text(sentence))\n",
        "        aug_labels.append(label)\n",
        "\n",
        "df_aug = pd.DataFrame({\n",
        "    \"Category\": aug_labels,\n",
        "    \"Cleaned_Sentence\": aug_sentences\n",
        "})\n",
        "\n",
        "df_aug = shuffle(df_aug, random_state=42)\n",
        "\n",
        "rf_vectorizer = TfidfVectorizer(max_features=5000, ngram_range=(1,2))\n",
        "X = rf_vectorizer.fit_transform(df_aug['Cleaned_Sentence'])\n",
        "y = df_aug['Category']\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "rf_model = RandomForestClassifier(\n",
        "    n_estimators=200,\n",
        "    random_state=42,\n",
        "    class_weight='balanced'\n",
        ")\n",
        "\n",
        "rf_model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = rf_model.predict(X_test)\n",
        "\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "recall = recall_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "f1 = f1_score(y_test, y_pred, average='weighted', zero_division=0)\n",
        "\n",
        "print(f\"Accuracy:  {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall:    {recall:.4f}\")\n",
        "print(f\"F1 Score:  {f1:.4f}\")\n",
        "\n",
        "def predict_text(text):\n",
        "    cleaned = preprocess_text(text)\n",
        "    vec = rf_vectorizer.transform([cleaned])\n",
        "    pred = rf_model.predict(vec)[0]\n",
        "    pred_prob = max(rf_model.predict_proba(vec)[0])\n",
        "\n",
        "    tokens = set(cleaned.split())\n",
        "\n",
        "    if len(tokens & positive_words) > 0:\n",
        "        pred = 'Non-depression'\n",
        "\n",
        "    if len(tokens & negative_words) > 0 and pred != 'Non-depression':\n",
        "        pred_prob = min(1.0, pred_prob + 0.1)\n",
        "\n",
        "    return pred, pred_prob\n",
        "\n",
        "while True:\n",
        "    text_input = input(\"Enter a Banglish sentence (or type 'exit' to quit): \")\n",
        "    if text_input.lower() == 'exit':\n",
        "        break\n",
        "    if text_input.strip() == \"\":\n",
        "        continue\n",
        "    prediction, confidence = predict_text(text_input)\n",
        "    print(f\"Predicted Category: {prediction}\")\n",
        "    print(f\"Confidence Score:   {confidence:.2f}\")\n",
        "    print()\n"
      ],
      "metadata": {
        "id": "2JDj6pDfmQle",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6a25be0a-3bc1-4324-fecd-0878845f59ff"
      },
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accuracy:  0.9609\n",
            "Precision: 0.9609\n",
            "Recall:    0.9609\n",
            "F1 Score:  0.9609\n",
            "Enter a Banglish sentence (or type 'exit' to quit): exit\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Saving the trained model"
      ],
      "metadata": {
        "id": "CfCjcmvqppNE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n"
      ],
      "metadata": {
        "id": "b36LTnT2fb7L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "metrics = {\n",
        "    \"Accuracy\": accuracy,\n",
        "    \"Precision\": precision,\n",
        "    \"Recall\": recall,\n",
        "    \"F1 Score\": f1\n",
        "}\n",
        "\n",
        "with open(\"banglish_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((rf_model, vectorizer, metrics), f)\n",
        "\n"
      ],
      "metadata": {
        "id": "WSGCn3UXCI8U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Loading the trained model"
      ],
      "metadata": {
        "id": "W415CA6jEKrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --upgrade streamlit blinker --ignore-installed\n"
      ],
      "metadata": {
        "id": "IO7wS_3dJgdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import pickle\n",
        "import numpy as np\n",
        "import re\n",
        "import emoji\n",
        "\n",
        "st.set_page_config(page_title=\"Banglish Depression Detector\", layout=\"wide\")\n",
        "st.title(\"🧠 Banglish Depression Detection (ML)\")\n",
        "\n",
        "# ------------------- PREPROCESS FUNCTION -------------------\n",
        "def preprocess_text(text):\n",
        "    eng_stop = {\"i\",\"me\",\"my\",\"myself\",\"we\",\"our\",\"ours\",\"ourselves\",\"you\",\"your\",\"yours\",\n",
        "                \"yourself\",\"yourselves\",\"he\",\"him\",\"his\",\"himself\",\"she\",\"her\",\"hers\",\n",
        "                \"herself\",\"it\",\"its\",\"itself\",\"they\",\"them\",\"their\",\"theirs\",\"themselves\",\n",
        "                \"what\",\"which\",\"who\",\"whom\",\"this\",\"that\",\"these\",\"those\",\"am\",\"is\",\"are\",\n",
        "                \"was\",\"were\",\"be\",\"been\",\"being\",\"have\",\"has\",\"had\",\"having\",\"do\",\"does\",\n",
        "                \"did\",\"doing\",\"a\",\"an\",\"the\",\"and\",\"but\",\"if\",\"or\",\"because\",\"as\",\"until\",\n",
        "                \"while\",\"of\",\"at\",\"by\",\"for\",\"with\",\"about\",\"against\",\"between\",\"into\",\n",
        "                \"through\",\"during\",\"before\",\"after\",\"above\",\"below\",\"to\",\"from\",\"up\",\"down\",\n",
        "                \"in\",\"out\",\"on\",\"off\",\"over\",\"under\",\"again\",\"further\",\"then\",\"once\",\"here\",\n",
        "                \"there\",\"when\",\"where\",\"why\",\"how\",\"all\",\"any\",\"both\",\"each\",\"few\",\"more\",\n",
        "                \"most\",\"other\",\"some\",\"such\",\"no\",\"nor\",\"not\",\"only\",\"own\",\"same\",\"so\",\n",
        "                \"than\",\"too\",\"very\",\"s\",\"t\",\"can\",\"will\",\"just\",\"don\",\"should\",\"now\"}\n",
        "\n",
        "    bn_stop = {\"ami\",\"tumi\",\"amra\",\"ache\",\"achhi\",\"kintu\",\"na\",\"ar\",\"shob\",\"ekta\",\n",
        "               \"kore\",\"shudhu\",\"amar\",\"tumar\",\"mone\",\"kotha\",\"ki\",\"kemon\",\n",
        "               \"tome\",\"tomar\",\"tara\",\"tarao\",\"taraor\",\"je\",\"sei\",\"ei\",\"oka\",\"ora\"}\n",
        "\n",
        "    stop_words = eng_stop.union(bn_stop)\n",
        "\n",
        "    text = str(text).lower()\n",
        "    text = emoji.demojize(text, delimiters=(\" \",\" \"))\n",
        "    text = re.sub(r\"http\\S+|www\\S+|https\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\S+@\\S+\", \"\", text)\n",
        "    text = re.sub(r\"\\d+\", \"\", text)\n",
        "    text = re.sub(r\"[^a-zA-Z\\u0980-\\u09FF ]+\", \" \", text)\n",
        "    text = re.sub(r\"(.)\\1{2,}\", r\"\\1\\1\", text)\n",
        "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
        "    tokens = [t for t in text.split() if t not in stop_words and len(t) > 1]\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "# ------------------- LOAD PICKLED MODEL -------------------\n",
        "@st.cache_resource\n",
        "def load_model():\n",
        "    with open(\"banglish_model.pkl\", \"rb\") as f:\n",
        "        model, vectorizer, metrics = pickle.load(f)\n",
        "    return model, vectorizer, metrics\n",
        "\n",
        "model, vectorizer, metrics = load_model()\n",
        "\n",
        "# ------------------- SIDEBAR -------------------\n",
        "st.sidebar.header(\"Model Performance on Test Set\")\n",
        "for k, v in metrics.items():\n",
        "    st.sidebar.write(f\"**{k}:** {v:.4f}\")\n",
        "\n",
        "st.sidebar.markdown(\"---\")\n",
        "st.sidebar.info(\"Trained on Banglish dataset with stopwords removed, TF-IDF + Random Forest. Includes augmentation & positive/negative word tweaks.\")\n",
        "\n",
        "# ------------------- MAIN PANEL -------------------\n",
        "st.subheader(\"Enter a Banglish sentence for prediction:\")\n",
        "\n",
        "text_input = st.text_area(\"\")\n",
        "\n",
        "if st.button(\"Predict\") and text_input.strip():\n",
        "    clean = preprocess_text(text_input)\n",
        "    vec = vectorizer.transform([clean])\n",
        "    pred = model.predict(vec)[0]\n",
        "    prob = max(model.predict_proba(vec)[0])\n",
        "\n",
        "    # Positive/Negative words adjustment\n",
        "    positive_words = {'moja','happy','bhalo','fun','sundor','friend','party','mojar'}\n",
        "    negative_words = {'dukho','kharaap','niras','lonely','stress','dukhi','depressed','chinta','udasin'}\n",
        "    tokens = set(clean.split())\n",
        "    if len(tokens & positive_words) > 0:\n",
        "        pred = 'Non-depression'\n",
        "    if len(tokens & negative_words) > 0 and pred != 'Non-depression':\n",
        "        prob = min(1.0, prob + 0.1)\n",
        "\n",
        "    st.success(f\"**Prediction:** {pred}\")\n",
        "    st.info(f\"**Confidence Score:** {prob:.2f}\")\n",
        "\n",
        "    # ------------------- TOP TF-IDF WORDS -------------------\n",
        "    st.subheader(\"Top words contributing (TF-IDF):\")\n",
        "    feature_names = np.array(vectorizer.get_feature_names_out())\n",
        "    tfidf_vec = vec.toarray()[0]\n",
        "    top_idx = tfidf_vec.argsort()[-10:][::-1]\n",
        "    top_words = [(feature_names[i], tfidf_vec[i]) for i in top_idx if tfidf_vec[i] > 0]\n",
        "    if top_words:\n",
        "        for word, score in top_words:\n",
        "            st.write(f\"{word}: {score:.4f}\")\n",
        "    else:\n",
        "        st.write(\"No significant words found.\")\n"
      ],
      "metadata": {
        "id": "vlA1XIU9HxPR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!streamlit run app.py --server.port 8501 --server.address 0.0.0.0 &>/content/streamlit.log &\n"
      ],
      "metadata": {
        "id": "i-K5F7JRH3vY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!wget -q https://github.com/cloudflare/cloudflared/releases/latest/download/cloudflared-linux-amd64\n",
        "!chmod +x cloudflared-linux-amd64\n"
      ],
      "metadata": {
        "id": "_nvG-JHxKkTD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!./cloudflared-linux-amd64 tunnel --url http://localhost:8501\n"
      ],
      "metadata": {
        "id": "u0oznLBjKpNr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "save and download model"
      ],
      "metadata": {
        "id": "HqCR5kncco9i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "# --- Logistic Regression ---\n",
        "with open(\"logistic_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((model, vectorizer, scaler), f)\n",
        "\n",
        "# --- Random Forest ---\n",
        "with open(\"rf_model.pkl\", \"wb\") as f:\n",
        "    pickle.dump((rf_model, vectorizer, stop_words), f)\n"
      ],
      "metadata": {
        "id": "Hi8eyGQRcmGx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# --- LSTM ---\n",
        "lstm_model.save(\"lstm_model.h5\")\n",
        "\n",
        "# --- ANN/MLP ---\n",
        "mlp_model.save(\"mlp_model.h5\")\n"
      ],
      "metadata": {
        "id": "qJp0x4Ihk162"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}